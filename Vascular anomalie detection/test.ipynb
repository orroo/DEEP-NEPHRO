{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a083643a",
   "metadata": {},
   "source": [
    " on utilise ici la bibliothèque diffusers (de Hugging Face) pour faire du Stable Diffusion Image-to-Image (img2img).\n",
    "\n",
    "Plus précisément :\n",
    "\n",
    "Modèle utilisé : \"runwayml/stable-diffusion-v1-5\", un modèle de Stable Diffusion v1.5.\n",
    "\n",
    "Pipeline utilisé : StableDiffusionImg2ImgPipeline, qui sert à transformer une image existante en nouvelle image guidée par un prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0649d5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amina\\sd-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 22.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 image(s) trouvée(s) dans C:/Users/Amina/Desktop/severe pain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [07:05<00:00, 11.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] Variation 1 sauvegardée : C:/Users/Amina/Desktop/severe pain/generated\\001_jpg.rf.a6e4c53c2f970d5e9547e2bdd2097616_gen_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [06:57<00:00, 11.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] Variation 2 sauvegardée : C:/Users/Amina/Desktop/severe pain/generated\\001_jpg.rf.a6e4c53c2f970d5e9547e2bdd2097616_gen_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [06:57<00:00, 11.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] Variation 3 sauvegardée : C:/Users/Amina/Desktop/severe pain/generated\\001_jpg.rf.a6e4c53c2f970d5e9547e2bdd2097616_gen_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [07:09<00:00, 11.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] Variation 4 sauvegardée : C:/Users/Amina/Desktop/severe pain/generated\\001_jpg.rf.a6e4c53c2f970d5e9547e2bdd2097616_gen_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [07:19<00:00, 11.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] Variation 5 sauvegardée : C:/Users/Amina/Desktop/severe pain/generated\\001_jpg.rf.a6e4c53c2f970d5e9547e2bdd2097616_gen_5.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [07:01<00:00, 11.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/100] Variation 1 sauvegardée : C:/Users/Amina/Desktop/severe pain/generated\\002_jpg.rf.325a0d02653bd6e60a00784f485920ce_gen_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [06:58<00:00, 11.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/100] Variation 2 sauvegardée : C:/Users/Amina/Desktop/severe pain/generated\\002_jpg.rf.325a0d02653bd6e60a00784f485920ce_gen_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [06:52<00:00, 11.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/100] Variation 3 sauvegardée : C:/Users/Amina/Desktop/severe pain/generated\\002_jpg.rf.325a0d02653bd6e60a00784f485920ce_gen_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [07:05<00:00, 11.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/100] Variation 4 sauvegardée : C:/Users/Amina/Desktop/severe pain/generated\\002_jpg.rf.325a0d02653bd6e60a00784f485920ce_gen_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [07:01<00:00, 11.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/100] Variation 5 sauvegardée : C:/Users/Amina/Desktop/severe pain/generated\\002_jpg.rf.325a0d02653bd6e60a00784f485920ce_gen_5.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [06:55<00:00, 11.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/100] Variation 1 sauvegardée : C:/Users/Amina/Desktop/severe pain/generated\\003_jpg.rf.0b581e515c39c9539d2b03811b4fafbc_gen_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [06:49<00:00, 11.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/100] Variation 2 sauvegardée : C:/Users/Amina/Desktop/severe pain/generated\\003_jpg.rf.0b581e515c39c9539d2b03811b4fafbc_gen_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [42:41<00:00, 69.22s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/100] Variation 3 sauvegardée : C:/Users/Amina/Desktop/severe pain/generated\\003_jpg.rf.0b581e515c39c9539d2b03811b4fafbc_gen_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [06:52<00:00, 11.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/100] Variation 4 sauvegardée : C:/Users/Amina/Desktop/severe pain/generated\\003_jpg.rf.0b581e515c39c9539d2b03811b4fafbc_gen_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 4/37 [00:54<06:46, 12.31s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Charger le modèle\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float32,\n",
    "    use_safetensors=True,\n",
    ").to(\"cpu\")  # Change en \"cuda\" si tu as un GPU compatible\n",
    "\n",
    "# 2. Fonction pour charger une image\n",
    "def load_image(image_path):\n",
    "    try:\n",
    "        return Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement de l'image {image_path} : {e}\")\n",
    "        return None\n",
    "\n",
    "# 3. Générer une image à partir d'une image et d'un prompt\n",
    "def generate_image(input_image, prompt, strength=0.75, guidance_scale=7.5):\n",
    "    input_image = input_image.resize((512, 512))\n",
    "    result = pipe(\n",
    "        prompt=prompt,\n",
    "        image=input_image,\n",
    "        strength=strength,\n",
    "        guidance_scale=guidance_scale\n",
    "    ).images[0]\n",
    "    return result\n",
    "\n",
    "# 4. Fonction principale : traiter un dossier entier\n",
    "def generate_from_folder(input_folder, output_folder, prompt, num_variations=3):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    image_files = [\n",
    "        f for f in os.listdir(input_folder)\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    ]\n",
    "    print(f\"{len(image_files)} image(s) trouvée(s) dans {input_folder}\")\n",
    "\n",
    "    for idx, image_file in enumerate(image_files):\n",
    "        input_path = os.path.join(input_folder, image_file)\n",
    "        base_image = load_image(input_path)\n",
    "\n",
    "        if base_image is None:\n",
    "            continue  # Ignore les fichiers non valides\n",
    "\n",
    "        for i in range(num_variations):\n",
    "            generated_img = generate_image(base_image, prompt)\n",
    "\n",
    "            output_filename = f\"{os.path.splitext(image_file)[0]}_gen_{i+1}.png\"\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "            generated_img.save(output_path)\n",
    "            print(f\"[{idx+1}/{len(image_files)}] Variation {i+1} sauvegardée : {output_path}\")\n",
    "\n",
    "# 5. Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"C:/Users/Amina/Desktop/severe pain\"\n",
    "    output_folder = \"C:/Users/Amina/Desktop/severe pain/generated\"\n",
    "    prompt = \"a realistic photo of a patient in severe pain, hospital background, medical lighting\"\n",
    "    generate_from_folder(input_folder, output_folder, prompt, num_variations=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c123e68-f988-4208-9e05-edb04a9eb1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python path: c:\\Users\\Amina\\sd-env\\Scripts\\python.exe\n",
      "NumPy version: 2.1.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy\n",
    "print(\"Python path:\", sys.executable)\n",
    "print(\"NumPy version:\", numpy.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89ff88",
   "metadata": {},
   "source": [
    "Ancienne version\t                            |Nouvelle version\n",
    "Forçait le CPU même si GPU dispo\t            |Utilise automatiquement CUDA si possible\n",
    "Pas de négatif prompt\t                        |Ajoute negative_prompt pour meilleure qualité\n",
    "Pas de protection contre doublons\t            |Ne régénère pas les images déjà existantes\n",
    "Pas de barre de progression\t                    |Ajoute tqdm pour voir où tu en es\n",
    "Force .float32\t                                |Optimise en .float16 sur GPU\n",
    "Moins paramétrable\t                            |Peut régler strength, guidance_scale, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be41b486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amina\\sd-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 15.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 image(s) trouvée(s) dans C:/Users/Amina/Desktop/severe pain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des images:   9%|▉         | 17/184 [00:00<00:00, 168.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Déjà générée, skip : 001_jpg.rf.a6e4c53c2f970d5e9547e2bdd2097616_gen_1.png\n",
      "Déjà générée, skip : 001_jpg.rf.a6e4c53c2f970d5e9547e2bdd2097616_gen_2.png\n",
      "Déjà générée, skip : 001_jpg.rf.a6e4c53c2f970d5e9547e2bdd2097616_gen_3.png\n",
      "Déjà générée, skip : 001_jpg.rf.a6e4c53c2f970d5e9547e2bdd2097616_gen_4.png\n",
      "Déjà générée, skip : 001_jpg.rf.a6e4c53c2f970d5e9547e2bdd2097616_gen_5.png\n",
      "Déjà générée, skip : 002_jpg.rf.325a0d02653bd6e60a00784f485920ce_gen_1.png\n",
      "Déjà générée, skip : 002_jpg.rf.325a0d02653bd6e60a00784f485920ce_gen_2.png\n",
      "Déjà générée, skip : 002_jpg.rf.325a0d02653bd6e60a00784f485920ce_gen_3.png\n",
      "Déjà générée, skip : 002_jpg.rf.325a0d02653bd6e60a00784f485920ce_gen_4.png\n",
      "Déjà générée, skip : 002_jpg.rf.325a0d02653bd6e60a00784f485920ce_gen_5.png\n",
      "Déjà générée, skip : 003_jpg.rf.0b581e515c39c9539d2b03811b4fafbc_gen_1.png\n",
      "Déjà générée, skip : 003_jpg.rf.0b581e515c39c9539d2b03811b4fafbc_gen_2.png\n",
      "Déjà générée, skip : 003_jpg.rf.0b581e515c39c9539d2b03811b4fafbc_gen_3.png\n",
      "Déjà générée, skip : 003_jpg.rf.0b581e515c39c9539d2b03811b4fafbc_gen_4.png\n",
      "Déjà générée, skip : 003_jpg.rf.0b581e515c39c9539d2b03811b4fafbc_gen_5.png\n",
      "Déjà générée, skip : 005_jpg.rf.7f1eeb63718cbce5f7454fd40b2d4c98_gen_1.png\n",
      "Déjà générée, skip : 005_jpg.rf.7f1eeb63718cbce5f7454fd40b2d4c98_gen_2.png\n",
      "Déjà générée, skip : 005_jpg.rf.7f1eeb63718cbce5f7454fd40b2d4c98_gen_3.png\n",
      "Déjà générée, skip : 005_jpg.rf.7f1eeb63718cbce5f7454fd40b2d4c98_gen_4.png\n",
      "Déjà générée, skip : 005_jpg.rf.7f1eeb63718cbce5f7454fd40b2d4c98_gen_5.png\n",
      "Déjà générée, skip : 008_jpg.rf.f3fc4d46734679ccc4b09c5d4fa375ee_gen_1.png\n",
      "Déjà générée, skip : 008_jpg.rf.f3fc4d46734679ccc4b09c5d4fa375ee_gen_2.png\n",
      "Déjà générée, skip : 008_jpg.rf.f3fc4d46734679ccc4b09c5d4fa375ee_gen_3.png\n",
      "Déjà générée, skip : 008_jpg.rf.f3fc4d46734679ccc4b09c5d4fa375ee_gen_4.png\n",
      "Déjà générée, skip : 008_jpg.rf.f3fc4d46734679ccc4b09c5d4fa375ee_gen_5.png\n",
      "Déjà générée, skip : 009_jpg.rf.7034924a468a7d351b3240a032e7deb6_gen_1.png\n",
      "Déjà générée, skip : 009_jpg.rf.7034924a468a7d351b3240a032e7deb6_gen_2.png\n",
      "Déjà générée, skip : 009_jpg.rf.7034924a468a7d351b3240a032e7deb6_gen_3.png\n",
      "Déjà générée, skip : 009_jpg.rf.7034924a468a7d351b3240a032e7deb6_gen_4.png\n",
      "Déjà générée, skip : 009_jpg.rf.7034924a468a7d351b3240a032e7deb6_gen_5.png\n",
      "Déjà générée, skip : 010_jpg.rf.1a3dac6e5ff6d4aaf574ba0698fca038_gen_1.png\n",
      "Déjà générée, skip : 010_jpg.rf.1a3dac6e5ff6d4aaf574ba0698fca038_gen_2.png\n",
      "Déjà générée, skip : 010_jpg.rf.1a3dac6e5ff6d4aaf574ba0698fca038_gen_3.png\n",
      "Déjà générée, skip : 010_jpg.rf.1a3dac6e5ff6d4aaf574ba0698fca038_gen_4.png\n",
      "Déjà générée, skip : 010_jpg.rf.1a3dac6e5ff6d4aaf574ba0698fca038_gen_5.png\n",
      "Déjà générée, skip : 015_jpg.rf.bbe3075edc3a7f6c6be842fc8ccea001_gen_1.png\n",
      "Déjà générée, skip : 015_jpg.rf.bbe3075edc3a7f6c6be842fc8ccea001_gen_2.png\n",
      "Déjà générée, skip : 015_jpg.rf.bbe3075edc3a7f6c6be842fc8ccea001_gen_3.png\n",
      "Déjà générée, skip : 015_jpg.rf.bbe3075edc3a7f6c6be842fc8ccea001_gen_4.png\n",
      "Déjà générée, skip : 015_jpg.rf.bbe3075edc3a7f6c6be842fc8ccea001_gen_5.png\n",
      "Déjà générée, skip : 015_jpg.rf.f13e420e0f4b4698c640a68a0ff8ef83_gen_1.png\n",
      "Déjà générée, skip : 015_jpg.rf.f13e420e0f4b4698c640a68a0ff8ef83_gen_2.png\n",
      "Déjà générée, skip : 015_jpg.rf.f13e420e0f4b4698c640a68a0ff8ef83_gen_3.png\n",
      "Déjà générée, skip : 015_jpg.rf.f13e420e0f4b4698c640a68a0ff8ef83_gen_4.png\n",
      "Déjà générée, skip : 015_jpg.rf.f13e420e0f4b4698c640a68a0ff8ef83_gen_5.png\n",
      "Déjà générée, skip : 017_jpg.rf.0a7a65068bbcf99752f7862083dba45b_gen_1.png\n",
      "Déjà générée, skip : 017_jpg.rf.0a7a65068bbcf99752f7862083dba45b_gen_2.png\n",
      "Déjà générée, skip : 017_jpg.rf.0a7a65068bbcf99752f7862083dba45b_gen_3.png\n",
      "Déjà générée, skip : 017_jpg.rf.0a7a65068bbcf99752f7862083dba45b_gen_4.png\n",
      "Déjà générée, skip : 017_jpg.rf.0a7a65068bbcf99752f7862083dba45b_gen_5.png\n",
      "Déjà générée, skip : 020_jpg.rf.c8a73802e0b435ea5dd9e040be9d30b5_gen_1.png\n",
      "Déjà générée, skip : 020_jpg.rf.c8a73802e0b435ea5dd9e040be9d30b5_gen_2.png\n",
      "Déjà générée, skip : 020_jpg.rf.c8a73802e0b435ea5dd9e040be9d30b5_gen_3.png\n",
      "Déjà générée, skip : 020_jpg.rf.c8a73802e0b435ea5dd9e040be9d30b5_gen_4.png\n",
      "Déjà générée, skip : 020_jpg.rf.c8a73802e0b435ea5dd9e040be9d30b5_gen_5.png\n",
      "Déjà générée, skip : 022_jpg.rf.fd52de5ee6b3d77d7826d026d6f2d378_gen_1.png\n",
      "Déjà générée, skip : 022_jpg.rf.fd52de5ee6b3d77d7826d026d6f2d378_gen_2.png\n",
      "Déjà générée, skip : 022_jpg.rf.fd52de5ee6b3d77d7826d026d6f2d378_gen_3.png\n",
      "Déjà générée, skip : 022_jpg.rf.fd52de5ee6b3d77d7826d026d6f2d378_gen_4.png\n",
      "Déjà générée, skip : 022_jpg.rf.fd52de5ee6b3d77d7826d026d6f2d378_gen_5.png\n",
      "Déjà générée, skip : 023_jpg.rf.a23e24b2f4004199d2e32122f77c3a18_gen_1.png\n",
      "Déjà générée, skip : 023_jpg.rf.a23e24b2f4004199d2e32122f77c3a18_gen_2.png\n",
      "Déjà générée, skip : 023_jpg.rf.a23e24b2f4004199d2e32122f77c3a18_gen_3.png\n",
      "Déjà générée, skip : 023_jpg.rf.a23e24b2f4004199d2e32122f77c3a18_gen_4.png\n",
      "Déjà générée, skip : 023_jpg.rf.a23e24b2f4004199d2e32122f77c3a18_gen_5.png\n",
      "Déjà générée, skip : 024_jpg.rf.ab7ca758d5989d94c20735fd137e4cf4_gen_1.png\n",
      "Déjà générée, skip : 024_jpg.rf.ab7ca758d5989d94c20735fd137e4cf4_gen_2.png\n",
      "Déjà générée, skip : 024_jpg.rf.ab7ca758d5989d94c20735fd137e4cf4_gen_3.png\n",
      "Déjà générée, skip : 024_jpg.rf.ab7ca758d5989d94c20735fd137e4cf4_gen_4.png\n",
      "Déjà générée, skip : 024_jpg.rf.ab7ca758d5989d94c20735fd137e4cf4_gen_5.png\n",
      "Déjà générée, skip : 029_jpg.rf.ffd0abb25cc33e047ec147134f56c6fb_gen_1.png\n",
      "Déjà générée, skip : 029_jpg.rf.ffd0abb25cc33e047ec147134f56c6fb_gen_2.png\n",
      "Déjà générée, skip : 029_jpg.rf.ffd0abb25cc33e047ec147134f56c6fb_gen_3.png\n",
      "Déjà générée, skip : 029_jpg.rf.ffd0abb25cc33e047ec147134f56c6fb_gen_4.png\n",
      "Déjà générée, skip : 029_jpg.rf.ffd0abb25cc33e047ec147134f56c6fb_gen_5.png\n",
      "Déjà générée, skip : 035_jpg.rf.dfcf1e3f191a37bc3589cf0f95d477e1_gen_1.png\n",
      "Déjà générée, skip : 035_jpg.rf.dfcf1e3f191a37bc3589cf0f95d477e1_gen_2.png\n",
      "Déjà générée, skip : 035_jpg.rf.dfcf1e3f191a37bc3589cf0f95d477e1_gen_3.png\n",
      "Déjà générée, skip : 035_jpg.rf.dfcf1e3f191a37bc3589cf0f95d477e1_gen_4.png\n",
      "Déjà générée, skip : 035_jpg.rf.dfcf1e3f191a37bc3589cf0f95d477e1_gen_5.png\n",
      "Déjà générée, skip : 037_jpg.rf.c1ea81ca8ba73836899f8c3792044595_gen_1.png\n",
      "Déjà générée, skip : 037_jpg.rf.c1ea81ca8ba73836899f8c3792044595_gen_2.png\n",
      "Déjà générée, skip : 037_jpg.rf.c1ea81ca8ba73836899f8c3792044595_gen_3.png\n",
      "Déjà générée, skip : 037_jpg.rf.c1ea81ca8ba73836899f8c3792044595_gen_4.png\n",
      "Déjà générée, skip : 037_jpg.rf.c1ea81ca8ba73836899f8c3792044595_gen_5.png\n",
      "Déjà générée, skip : 045_jpg.rf.cf1a45b445db7a17934993e5e183ae52_gen_1.png\n",
      "Déjà générée, skip : 045_jpg.rf.cf1a45b445db7a17934993e5e183ae52_gen_2.png\n",
      "Déjà générée, skip : 045_jpg.rf.cf1a45b445db7a17934993e5e183ae52_gen_3.png\n",
      "Déjà générée, skip : 045_jpg.rf.cf1a45b445db7a17934993e5e183ae52_gen_4.png\n",
      "Déjà générée, skip : 045_jpg.rf.cf1a45b445db7a17934993e5e183ae52_gen_5.png\n",
      "Déjà générée, skip : 046_jpg.rf.e3a3f04b8f6ab5b7832f8c429750e529_gen_1.png\n",
      "Déjà générée, skip : 046_jpg.rf.e3a3f04b8f6ab5b7832f8c429750e529_gen_2.png\n",
      "Déjà générée, skip : 046_jpg.rf.e3a3f04b8f6ab5b7832f8c429750e529_gen_3.png\n",
      "Déjà générée, skip : 046_jpg.rf.e3a3f04b8f6ab5b7832f8c429750e529_gen_4.png\n",
      "Déjà générée, skip : 046_jpg.rf.e3a3f04b8f6ab5b7832f8c429750e529_gen_5.png\n",
      "Déjà générée, skip : 051_jpg.rf.9a22c751d1af46df7cd751bebc00276a_gen_1.png\n",
      "Déjà générée, skip : 051_jpg.rf.9a22c751d1af46df7cd751bebc00276a_gen_2.png\n",
      "Déjà générée, skip : 051_jpg.rf.9a22c751d1af46df7cd751bebc00276a_gen_3.png\n",
      "Déjà générée, skip : 051_jpg.rf.9a22c751d1af46df7cd751bebc00276a_gen_4.png\n",
      "Déjà générée, skip : 051_jpg.rf.9a22c751d1af46df7cd751bebc00276a_gen_5.png\n",
      "Déjà générée, skip : 052_jpg.rf.cb8f5d7189fdc44bd6007c3f91c51e2d_gen_1.png\n",
      "Déjà générée, skip : 052_jpg.rf.cb8f5d7189fdc44bd6007c3f91c51e2d_gen_2.png\n",
      "Déjà générée, skip : 052_jpg.rf.cb8f5d7189fdc44bd6007c3f91c51e2d_gen_3.png\n",
      "Déjà générée, skip : 052_jpg.rf.cb8f5d7189fdc44bd6007c3f91c51e2d_gen_4.png\n",
      "Déjà générée, skip : 052_jpg.rf.cb8f5d7189fdc44bd6007c3f91c51e2d_gen_5.png\n",
      "Déjà générée, skip : 074_jpg.rf.25e205e1dbb9e7f966264dcbd7dbf74a_gen_1.png\n",
      "Déjà générée, skip : 074_jpg.rf.25e205e1dbb9e7f966264dcbd7dbf74a_gen_2.png\n",
      "Déjà générée, skip : 074_jpg.rf.25e205e1dbb9e7f966264dcbd7dbf74a_gen_3.png\n",
      "Déjà générée, skip : 074_jpg.rf.25e205e1dbb9e7f966264dcbd7dbf74a_gen_4.png\n",
      "Déjà générée, skip : 074_jpg.rf.25e205e1dbb9e7f966264dcbd7dbf74a_gen_5.png\n",
      "Déjà générée, skip : 075_jpg.rf.c42af7666d4ac9c52886990188954f8e_gen_1.png\n",
      "Déjà générée, skip : 075_jpg.rf.c42af7666d4ac9c52886990188954f8e_gen_2.png\n",
      "Déjà générée, skip : 075_jpg.rf.c42af7666d4ac9c52886990188954f8e_gen_3.png\n",
      "Déjà générée, skip : 075_jpg.rf.c42af7666d4ac9c52886990188954f8e_gen_4.png\n",
      "Déjà générée, skip : 075_jpg.rf.c42af7666d4ac9c52886990188954f8e_gen_5.png\n",
      "Déjà générée, skip : 084_jpg.rf.f27278ded1c4d39bb8eef993f250370a_gen_1.png\n",
      "Déjà générée, skip : 084_jpg.rf.f27278ded1c4d39bb8eef993f250370a_gen_2.png\n",
      "Déjà générée, skip : 084_jpg.rf.f27278ded1c4d39bb8eef993f250370a_gen_3.png\n",
      "Déjà générée, skip : 084_jpg.rf.f27278ded1c4d39bb8eef993f250370a_gen_4.png\n",
      "Déjà générée, skip : 084_jpg.rf.f27278ded1c4d39bb8eef993f250370a_gen_5.png\n",
      "Déjà générée, skip : 100_jpg.rf.c6c0b69800c4134b68659ff7b8f65114_gen_1.png\n",
      "Déjà générée, skip : 100_jpg.rf.c6c0b69800c4134b68659ff7b8f65114_gen_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27 [00:11<?, ?it/s]\n",
      "Traitement des images:  13%|█▎        | 24/184 [00:15<01:46,  1.51it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 78\u001b[0m\n\u001b[0;32m     75\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma highly detailed, realistic photo of a patient experiencing severe pain, hospital environment, professional medical lighting, emotional expression, clinical background, 8k resolution, natural skin texture, sharp focus, cinematic lighting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m     negative_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblurry, bad anatomy, disfigured, extra limbs, extra fingers, missing fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, cross-eye, watermark, signature, text, low quality, bad proportions, cloned face, long neck, ugly, tiling, artifacts, out of frame, bad hands, bad feet, twisted limbs, bad eyes, unbalanced composition\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 78\u001b[0m     \u001b[43mgenerate_from_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_variations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.55\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# conserve bien la base de ton image\u001b[39;49;00m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.5\u001b[39;49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# suit bien ta description\u001b[39;49;00m\n\u001b[0;32m     86\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 61\u001b[0m, in \u001b[0;36mgenerate_from_folder\u001b[1;34m(input_folder, output_folder, prompt, negative_prompt, num_variations, strength, guidance_scale)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDéjà générée, skip : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m generated_img \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m generated_img\u001b[38;5;241m.\u001b[39msave(output_path)\n",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m, in \u001b[0;36mgenerate_image\u001b[1;34m(input_image, prompt, negative_prompt, strength, guidance_scale)\u001b[0m\n\u001b[0;32m     25\u001b[0m input_image \u001b[38;5;241m=\u001b[39m input_image\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m))\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device) \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 27\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\pipelines\\stable_diffusion\\pipeline_stable_diffusion_img2img.py:1101\u001b[0m, in \u001b[0;36mStableDiffusionImg2ImgPipeline.__call__\u001b[1;34m(self, prompt, image, strength, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m latent_model_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mscale_model_input(latent_model_input, t)\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;66;03m# predict the noise residual\u001b[39;00m\n\u001b[1;32m-> 1101\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_classifier_free_guidance:\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\models\\unets\\unet_2d_condition.py:1214\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[1;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_adapter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(down_intrablock_additional_residuals) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1212\u001b[0m         additional_residuals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_residuals\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m down_intrablock_additional_residuals\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m-> 1214\u001b[0m     sample, res_samples \u001b[38;5;241m=\u001b[39m downsample_block(\n\u001b[0;32m   1215\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39msample,\n\u001b[0;32m   1216\u001b[0m         temb\u001b[38;5;241m=\u001b[39memb,\n\u001b[0;32m   1217\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1218\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1219\u001b[0m         cross_attention_kwargs\u001b[38;5;241m=\u001b[39mcross_attention_kwargs,\n\u001b[0;32m   1220\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   1221\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madditional_residuals,\n\u001b[0;32m   1222\u001b[0m     )\n\u001b[0;32m   1223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1224\u001b[0m     sample, res_samples \u001b[38;5;241m=\u001b[39m downsample_block(hidden_states\u001b[38;5;241m=\u001b[39msample, temb\u001b[38;5;241m=\u001b[39memb)\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\models\\unets\\unet_2d_blocks.py:1270\u001b[0m, in \u001b[0;36mCrossAttnDownBlock2D.forward\u001b[1;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask, additional_residuals)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1269\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb)\n\u001b[1;32m-> 1270\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;66;03m# apply additional residuals to the output of the last pair of resnet and attention blocks\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m additional_residuals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\models\\transformers\\transformer_2d.py:427\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[0;32m    416\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    417\u001b[0m             block,\n\u001b[0;32m    418\u001b[0m             hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    424\u001b[0m             class_labels,\n\u001b[0;32m    425\u001b[0m         )\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;66;03m# 3. Output\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_continuous:\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\models\\attention.py:514\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[0m\n\u001b[0;32m    511\u001b[0m cross_attention_kwargs \u001b[38;5;241m=\u001b[39m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m cross_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    512\u001b[0m gligen_kwargs \u001b[38;5;241m=\u001b[39m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgligen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 514\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn1(\n\u001b[0;32m    515\u001b[0m     norm_hidden_states,\n\u001b[0;32m    516\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monly_cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    517\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs,\n\u001b[0;32m    519\u001b[0m )\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mada_norm_zero\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    522\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m gate_msa\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m attn_output\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\models\\attention_processor.py:605\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    601\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_attention_kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are not expected by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    602\u001b[0m     )\n\u001b[0;32m    603\u001b[0m cross_attention_kwargs \u001b[38;5;241m=\u001b[39m {k: w \u001b[38;5;28;01mfor\u001b[39;00m k, w \u001b[38;5;129;01min\u001b[39;00m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m attn_parameters}\n\u001b[1;32m--> 605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    607\u001b[0m     hidden_states,\n\u001b[0;32m    608\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    609\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs,\n\u001b[0;32m    611\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\models\\attention_processor.py:3313\u001b[0m, in \u001b[0;36mAttnProcessor2_0.__call__\u001b[1;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3309\u001b[0m     key \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mnorm_k(key)\n\u001b[0;32m   3311\u001b[0m \u001b[38;5;66;03m# the output of sdp = (batch, num_heads, seq_len, head_dim)\u001b[39;00m\n\u001b[0;32m   3312\u001b[0m \u001b[38;5;66;03m# TODO: add support for attn.scale when we move to Torch 2.1\u001b[39;00m\n\u001b[1;32m-> 3313\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   3315\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3317\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attn\u001b[38;5;241m.\u001b[39mheads \u001b[38;5;241m*\u001b[39m head_dim)\n\u001b[0;32m   3318\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(query\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "# 1. Charger le modèle\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "# 2. Fonction pour charger une image\n",
    "def load_image(image_path):\n",
    "    try:\n",
    "        return Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement de {image_path} : {e}\")\n",
    "        return None\n",
    "\n",
    "# 3. Générer une image\n",
    "def generate_image(input_image, prompt, negative_prompt=\"\", strength=0.75, guidance_scale=7.5):\n",
    "    input_image = input_image.resize((512, 512))\n",
    "    with torch.autocast(device) if device == \"cuda\" else torch.no_grad():\n",
    "        result = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            image=input_image,\n",
    "            strength=strength,\n",
    "            guidance_scale=guidance_scale,\n",
    "        ).images[0]\n",
    "    return result\n",
    "\n",
    "# 4. Fonction principale\n",
    "def generate_from_folder(input_folder, output_folder, prompt, negative_prompt=\"\", num_variations=3, strength=0.75, guidance_scale=7.5):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    image_files = [\n",
    "        f for f in os.listdir(input_folder)\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    ]\n",
    "    print(f\"{len(image_files)} image(s) trouvée(s) dans {input_folder}\")\n",
    "\n",
    "    for idx, image_file in enumerate(tqdm(image_files, desc=\"Traitement des images\")):\n",
    "        input_path = os.path.join(input_folder, image_file)\n",
    "        base_image = load_image(input_path)\n",
    "\n",
    "        if base_image is None:\n",
    "            continue\n",
    "\n",
    "        for i in range(num_variations):\n",
    "            output_filename = f\"{os.path.splitext(image_file)[0]}_gen_{i+1}.png\"\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "            if os.path.exists(output_path):\n",
    "                print(f\"Déjà générée, skip : {output_filename}\")\n",
    "                continue\n",
    "\n",
    "            generated_img = generate_image(\n",
    "                base_image,\n",
    "                prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                strength=strength,\n",
    "                guidance_scale=guidance_scale,\n",
    "            )\n",
    "            generated_img.save(output_path)\n",
    "\n",
    "# 5. Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"C:/Users/Amina/Desktop/severe pain\"\n",
    "    output_folder = \"C:/Users/Amina/Desktop/severe pain/generated\"\n",
    "    \n",
    "    prompt = \"a highly detailed, realistic photo of a patient experiencing severe pain, hospital environment, professional medical lighting, emotional expression, clinical background, 8k resolution, natural skin texture, sharp focus, cinematic lighting\"\n",
    "    negative_prompt = \"blurry, bad anatomy, disfigured, extra limbs, extra fingers, missing fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, cross-eye, watermark, signature, text, low quality, bad proportions, cloned face, long neck, ugly, tiling, artifacts, out of frame, bad hands, bad feet, twisted limbs, bad eyes, unbalanced composition\"\n",
    "\n",
    "    generate_from_folder(\n",
    "    input_folder=input_folder,\n",
    "    output_folder=output_folder,\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_variations=5,\n",
    "    strength=0.55,            # conserve bien la base de ton image\n",
    "    guidance_scale=10.5       # suit bien ta description\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8502fb4",
   "metadata": {},
   "source": [
    "J’ai ajouté **ControlNet avec OpenPose** pour guider la génération par la **posture corporelle extraite automatiquement** de chaque image. Ça rend les expressions et poses **plus cohérentes** et **réalistes**, tout en conservant ton image de base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f7f680",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mopenpose\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# No \"build\" or \"Release\" needed!\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenpose\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pyopenpose \u001b[38;5;28;01mas\u001b[39;00m op\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openpose'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\openpose')  # No \"build\" or \"Release\" needed!\n",
    "from openpose import pyopenpose as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc2ca732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch lllyasviel/sd-controlnet-openpose: Can't load the model for 'lllyasviel/sd-controlnet-openpose'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'lllyasviel/sd-controlnet-openpose' is the correct path to a directory containing a file named diffusion_pytorch_model.safetensors\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading models: Can't load the model for 'lllyasviel/sd-controlnet-openpose'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'lllyasviel/sd-controlnet-openpose' is the correct path to a directory containing a file named diffusion_pytorch_model.bin\n",
      "Starting image processing...\n",
      "Found 98 images to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  15%|█▌        | 15/98 [00:00<00:00, 142.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 001_jpg.rf.a6e4c53c2f970d5e9547e2bdd2097616.jpg: name 'pose_detector' is not defined\n",
      "Error processing 002_jpg.rf.325a0d02653bd6e60a00784f485920ce.jpg: name 'pose_detector' is not defined\n",
      "Error processing 003_jpg.rf.0b581e515c39c9539d2b03811b4fafbc.jpg: name 'pose_detector' is not defined\n",
      "Error processing 005_jpg.rf.7f1eeb63718cbce5f7454fd40b2d4c98.jpg: name 'pose_detector' is not defined\n",
      "Error processing 008_jpg.rf.f3fc4d46734679ccc4b09c5d4fa375ee.jpg: name 'pose_detector' is not defined\n",
      "Error processing 009_jpg.rf.7034924a468a7d351b3240a032e7deb6.jpg: name 'pose_detector' is not defined\n",
      "Error processing 010_jpg.rf.1a3dac6e5ff6d4aaf574ba0698fca038.jpg: name 'pose_detector' is not defined\n",
      "Error processing 015_jpg.rf.bbe3075edc3a7f6c6be842fc8ccea001.jpg: name 'pose_detector' is not defined\n",
      "Error processing 015_jpg.rf.f13e420e0f4b4698c640a68a0ff8ef83.jpg: name 'pose_detector' is not defined\n",
      "Error processing 017_jpg.rf.0a7a65068bbcf99752f7862083dba45b.jpg: name 'pose_detector' is not defined\n",
      "Error processing 020_jpg.rf.c8a73802e0b435ea5dd9e040be9d30b5.jpg: name 'pose_detector' is not defined\n",
      "Error processing 022_jpg.rf.fd52de5ee6b3d77d7826d026d6f2d378.jpg: name 'pose_detector' is not defined\n",
      "Error processing 023_jpg.rf.a23e24b2f4004199d2e32122f77c3a18.jpg: name 'pose_detector' is not defined\n",
      "Error processing 024_jpg.rf.ab7ca758d5989d94c20735fd137e4cf4.jpg: name 'pose_detector' is not defined\n",
      "Error processing 029_jpg.rf.ffd0abb25cc33e047ec147134f56c6fb.jpg: name 'pose_detector' is not defined\n",
      "Error processing 035_jpg.rf.dfcf1e3f191a37bc3589cf0f95d477e1.jpg: name 'pose_detector' is not defined\n",
      "Error processing 037_jpg.rf.c1ea81ca8ba73836899f8c3792044595.jpg: name 'pose_detector' is not defined\n",
      "Error processing 045_jpg.rf.cf1a45b445db7a17934993e5e183ae52.jpg: name 'pose_detector' is not defined\n",
      "Error processing 046_jpg.rf.e3a3f04b8f6ab5b7832f8c429750e529.jpg: name 'pose_detector' is not defined\n",
      "Error processing 051_jpg.rf.9a22c751d1af46df7cd751bebc00276a.jpg: name 'pose_detector' is not defined\n",
      "Error processing 052_jpg.rf.cb8f5d7189fdc44bd6007c3f91c51e2d.jpg: name 'pose_detector' is not defined\n",
      "Error processing 074_jpg.rf.25e205e1dbb9e7f966264dcbd7dbf74a.jpg: name 'pose_detector' is not defined\n",
      "Error processing 075_jpg.rf.c42af7666d4ac9c52886990188954f8e.jpg: name 'pose_detector' is not defined\n",
      "Error processing 084_jpg.rf.f27278ded1c4d39bb8eef993f250370a.jpg: name 'pose_detector' is not defined\n",
      "Error processing 100_jpg.rf.c6c0b69800c4134b68659ff7b8f65114.jpg: name 'pose_detector' is not defined\n",
      "Error processing 110697494_jpg.rf.3593f51123f2b0287a86f9d7f0310f32.jpg: name 'pose_detector' is not defined\n",
      "Error processing 111_jpg.rf.69c90e9a8e0ca9a700965e4f588210bb.jpg: name 'pose_detector' is not defined\n",
      "Error processing 1181296252_jpg.rf.20ddd397be56ee0dfaebed6c717efbd4.jpg: name 'pose_detector' is not defined\n",
      "Error processing 121_jpg.rf.9adaa8036310f0945e0222e106a66f1b.jpg: name 'pose_detector' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  55%|█████▌    | 54/98 [00:00<00:00, 183.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 123_jpg.rf.325c00a1f4c1a8886ce9a9097a82a84b.jpg: name 'pose_detector' is not defined\n",
      "Error processing 174191543_png_jpg.rf.b15db5e74301c70d595f411fd16ae135.jpg: name 'pose_detector' is not defined\n",
      "Error processing 1879775281_jpg.rf.778d3528ceb978a38a4a865a983255ab.jpg: name 'pose_detector' is not defined\n",
      "Error processing 323598674_jpg.rf.3b238d2dbc286e5bc959ace56710d87c.jpg: name 'pose_detector' is not defined\n",
      "Error processing 436079707_jpg.rf.95da9631a279fd320270ab8450447d41.jpg: name 'pose_detector' is not defined\n",
      "Error processing 766077652_jpg.rf.1c68e09ffbf4a18aceb04136520b636f.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_111.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_112.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_113.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_114.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_1147.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_115.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_12.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_1214.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_1215.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_1237.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_1238.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_1243.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_1256.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_13.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_14.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_15.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_16.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_27.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_28.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_29.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_30.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_31.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_440.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_45.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_46.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_47.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_479.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_48.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_49.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_496.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_50.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_505.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_506.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_507.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_508.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_509.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_51.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_52.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_53.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_54.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_55.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_56.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_57.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_79.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_80.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_81.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_82.jpg: name 'pose_detector' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 98/98 [00:00<00:00, 205.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image_83.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_84.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_85.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_86.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_87.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_88.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_89.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_90.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_91.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_92.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_93.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_94.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_95.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_96.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_97.jpg: name 'pose_detector' is not defined\n",
      "Error processing image_98.jpg: name 'pose_detector' is not defined\n",
      "\n",
      "Processing completed in 0.01 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "from controlnet_aux import OpenposeDetector\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === Configuration ===\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# === Model Loading ===\n",
    "try:\n",
    "    print(\"Loading models...\")\n",
    "    \n",
    "    # Load ControlNet\n",
    "    controlnet = ControlNetModel.from_pretrained(\n",
    "        \"lllyasviel/sd-controlnet-openpose\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Load Stable Diffusion pipeline\n",
    "    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        controlnet=controlnet,\n",
    "        torch_dtype=torch.float16,\n",
    "        safety_checker=None\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize pose detector - THIS IS THE CORRECT WAY\n",
    "    pose_detector = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# === Image Processing ===\n",
    "def process_image(input_path, output_path, prompt, negative_prompt=\"\"):\n",
    "    try:\n",
    "        # Load and prepare image\n",
    "        image = Image.open(input_path).convert(\"RGB\").resize((512, 512))\n",
    "        \n",
    "        # Get pose estimation - USING THE PROPER DETECTOR\n",
    "        pose_image = pose_detector(image)\n",
    "        \n",
    "        # Generate image\n",
    "        result = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            image=pose_image,  # Use pose image as control\n",
    "            guidance_scale=9.0,\n",
    "            num_inference_steps=20\n",
    "        ).images[0]\n",
    "        \n",
    "        result.save(output_path)\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {os.path.basename(input_path)}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# === Main Processing ===\n",
    "def process_folder(input_folder, output_folder, prompt, negative_prompt=\"\"):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(input_folder) \n",
    "                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    # Process each image\n",
    "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "        input_path = os.path.join(input_folder, image_file)\n",
    "        output_path = os.path.join(output_folder, f\"processed_{image_file}\")\n",
    "        \n",
    "        if not os.path.exists(output_path):\n",
    "            process_image(input_path, output_path, prompt, negative_prompt)\n",
    "        else:\n",
    "            print(f\"Skipping {image_file} (already processed)\")\n",
    "\n",
    "# === Execution ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    input_folder = \"C:/Users/Amina/Desktop/severe pain\"\n",
    "    output_folder = \"C:/Users/Amina/Desktop/severe pain/output\"\n",
    "    \n",
    "    # Prompts\n",
    "    prompt = (\"realistic photo of person in severe pain, \"\n",
    "             \"detailed facial expression, hospital setting\")\n",
    "    negative_prompt = (\"blurry, deformed, cartoon, anime, \"\n",
    "                      \"text, watermark, bad anatomy\")\n",
    "    \n",
    "    # Start processing\n",
    "    print(\"Starting image processing...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    process_folder(input_folder, output_folder, prompt, negative_prompt)\n",
    "    \n",
    "    # Completion message\n",
    "    duration = (time.time() - start_time) / 60\n",
    "    print(f\"\\nProcessing completed in {duration:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d171f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting controlnet-aux\n",
      "  Using cached controlnet_aux-0.0.9-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\amina\\sd-env\\lib\\site-packages (from controlnet-aux) (2.1.0+cpu)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\amina\\sd-env\\lib\\site-packages (from controlnet-aux) (8.6.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\amina\\sd-env\\lib\\site-packages (from controlnet-aux) (0.30.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\amina\\sd-env\\lib\\site-packages (from controlnet-aux) (1.15.2)\n",
      "Collecting opencv-python-headless (from controlnet-aux)\n",
      "  Using cached opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\amina\\sd-env\\lib\\site-packages (from controlnet-aux) (3.18.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\amina\\sd-env\\lib\\site-packages (from controlnet-aux) (1.26.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\amina\\sd-env\\lib\\site-packages (from controlnet-aux) (11.2.1)\n",
      "Collecting einops (from controlnet-aux)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: torchvision in c:\\users\\amina\\sd-env\\lib\\site-packages (from controlnet-aux) (0.16.0+cpu)\n",
      "Collecting timm<=0.6.7 (from controlnet-aux)\n",
      "  Using cached timm-0.6.7-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting scikit-image (from controlnet-aux)\n",
      "  Using cached scikit_image-0.25.2-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\amina\\sd-env\\lib\\site-packages (from torch->controlnet-aux) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\amina\\sd-env\\lib\\site-packages (from torch->controlnet-aux) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\amina\\sd-env\\lib\\site-packages (from torch->controlnet-aux) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\amina\\sd-env\\lib\\site-packages (from torch->controlnet-aux) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\amina\\sd-env\\lib\\site-packages (from torch->controlnet-aux) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\amina\\sd-env\\lib\\site-packages (from huggingface-hub->controlnet-aux) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\amina\\sd-env\\lib\\site-packages (from huggingface-hub->controlnet-aux) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\amina\\sd-env\\lib\\site-packages (from huggingface-hub->controlnet-aux) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\amina\\sd-env\\lib\\site-packages (from huggingface-hub->controlnet-aux) (4.67.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\amina\\sd-env\\lib\\site-packages (from importlib-metadata->controlnet-aux) (3.21.0)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image->controlnet-aux)\n",
      "  Using cached imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->controlnet-aux)\n",
      "  Using cached tifffile-2025.3.30-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->controlnet-aux)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\amina\\sd-env\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->controlnet-aux) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\amina\\sd-env\\lib\\site-packages (from jinja2->torch->controlnet-aux) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\amina\\sd-env\\lib\\site-packages (from requests->huggingface-hub->controlnet-aux) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amina\\sd-env\\lib\\site-packages (from requests->huggingface-hub->controlnet-aux) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\amina\\sd-env\\lib\\site-packages (from requests->huggingface-hub->controlnet-aux) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amina\\sd-env\\lib\\site-packages (from requests->huggingface-hub->controlnet-aux) (2022.12.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\amina\\sd-env\\lib\\site-packages (from sympy->torch->controlnet-aux) (1.3.0)\n",
      "Using cached controlnet_aux-0.0.9-py3-none-any.whl (282 kB)\n",
      "Using cached timm-0.6.7-py3-none-any.whl (509 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (39.4 MB)\n",
      "Using cached scikit_image-0.25.2-cp310-cp310-win_amd64.whl (12.8 MB)\n",
      "Using cached imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached tifffile-2025.3.30-py3-none-any.whl (226 kB)\n",
      "Installing collected packages: tifffile, opencv-python-headless, lazy-loader, imageio, einops, scikit-image, timm, controlnet-aux\n",
      "Successfully installed controlnet-aux-0.0.9 einops-0.8.1 imageio-2.37.0 lazy-loader-0.4 opencv-python-headless-4.11.0.86 scikit-image-0.25.2 tifffile-2025.3.30 timm-0.6.7\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install controlnet-aux\n",
    "#pour importer des bib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc761439",
   "metadata": {},
   "source": [
    " CODE COMPLET : LoRA + ControlNet + Img2Img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4c3e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00, 12.35it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from checkpoint file for './loras\\mon_lora.safetensors' at './loras\\mon_lora.safetensors'. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\models\\model_loading_utils.py:176\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file, dduf_entries, disable_mmap, map_location)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msafetensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_extension \u001b[38;5;241m==\u001b[39m GGUF_FILE_EXTENSION:\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\safetensors\\torch.py:313\u001b[0m, in \u001b[0;36mload_file\u001b[1;34m(filename, device)\u001b[0m\n\u001b[0;32m    312\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[1;31mSafetensorError\u001b[0m: Error while deserializing header: HeaderTooSmall",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\models\\model_loading_utils.py:202\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file, dduf_entries, disable_mmap, map_location)\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    203\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to locate the file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is necessary to load this pretrained \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel. Make sure you have saved the model properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    205\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to locate the file ./loras\\mon_lora.safetensors which is necessary to load this pretrained model. Make sure you have saved the model properly.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 100\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 Starting pipeline...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 100\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m \u001b[43msetup_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     process_folder(pipe)\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎉 All done!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 35\u001b[0m, in \u001b[0;36msetup_pipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Load LoRA (Diffusers 0.18.0+)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(LORA_PATH):\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lora_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLORA_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLORA_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ LoRA loaded from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLORA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\loaders\\lora_pipeline.py:196\u001b[0m, in \u001b[0;36mStableDiffusionLoraLoaderMixin.load_lora_weights\u001b[1;34m(self, pretrained_model_name_or_path_or_dict, adapter_name, hotswap, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m     pretrained_model_name_or_path_or_dict \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path_or_dict\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# First, ensure that the checkpoint is a compatible one and can be successfully loaded.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m state_dict, network_alphas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_state_dict(pretrained_model_name_or_path_or_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    198\u001b[0m is_correct_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlora\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m key \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_correct_format:\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\loaders\\lora_pipeline.py:300\u001b[0m, in \u001b[0;36mStableDiffusionLoraLoaderMixin.lora_state_dict\u001b[1;34m(cls, pretrained_model_name_or_path_or_dict, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m     allow_pickle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    295\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_procs_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    298\u001b[0m }\n\u001b[1;32m--> 300\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43m_fetch_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path_or_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path_or_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m is_dora_scale_present \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdora_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m state_dict)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dora_scale_present:\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\loaders\\lora_base.py:263\u001b[0m, in \u001b[0;36m_fetch_state_dict\u001b[1;34m(pretrained_model_name_or_path_or_dict, weight_name, use_safetensors, local_files_only, cache_dir, force_download, proxies, token, revision, subfolder, user_agent, allow_pickle)\u001b[0m\n\u001b[0;32m    248\u001b[0m             weight_name \u001b[38;5;241m=\u001b[39m _best_guess_weight_name(\n\u001b[0;32m    249\u001b[0m                 pretrained_model_name_or_path_or_dict, file_extension\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m, local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only\n\u001b[0;32m    250\u001b[0m             )\n\u001b[0;32m    251\u001b[0m         model_file \u001b[38;5;241m=\u001b[39m _get_model_file(\n\u001b[0;32m    252\u001b[0m             pretrained_model_name_or_path_or_dict,\n\u001b[0;32m    253\u001b[0m             weights_name\u001b[38;5;241m=\u001b[39mweight_name \u001b[38;5;129;01mor\u001b[39;00m LORA_WEIGHT_NAME,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m             user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    262\u001b[0m         )\n\u001b[1;32m--> 263\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path_or_dict\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\models\\model_loading_utils.py:207\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file, dduf_entries, disable_mmap, map_location)\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    203\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to locate the file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is necessary to load this pretrained \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel. Make sure you have saved the model properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    205\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load weights from checkpoint file for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to load weights from checkpoint file for './loras\\mon_lora.safetensors' at './loras\\mon_lora.safetensors'. "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"  # Base model\n",
    "LORA_PATH = \"./loras/mon_lora.safetensors\"    # Path to your LoRA file\n",
    "\n",
    "# Image generation settings\n",
    "PROMPT = \"a realistic, emotional photo of a patient in visible severe pain, expressive face, hospital lighting\"\n",
    "NEGATIVE_PROMPT = \"blurry, distorted face, bad anatomy, extra limbs, text, watermark, low quality\"\n",
    "STRENGTH = 0.55       # How much to alter the input image (0=ignore, 1=completely change)\n",
    "GUIDANCE_SCALE = 10.0 # How closely to follow the prompt\n",
    "NUM_STEPS = 30        # Inference steps (20-50 is typical)\n",
    "\n",
    "# Paths (use raw strings or forward slashes)\n",
    "INPUT_FOLDER = r\"C:\\Users\\Amina\\Desktop\\severe pain\"\n",
    "OUTPUT_FOLDER = r\"C:\\Users\\Amina\\Desktop\\severe pain\\generated_lora\"\n",
    "NUM_VARIATIONS = 4    # Number of outputs per input image\n",
    "\n",
    "# ========== SETUP PIPELINE ==========\n",
    "def setup_pipeline():\n",
    "    # Load base model\n",
    "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "        safety_checker=None,  # Disable if you get NSFW filter false positives\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Load LoRA (Diffusers 0.18.0+)\n",
    "    if os.path.exists(LORA_PATH):\n",
    "        pipe.load_lora_weights(os.path.dirname(LORA_PATH), weight_name=os.path.basename(LORA_PATH))\n",
    "        print(f\"✅ LoRA loaded from {LORA_PATH}\")\n",
    "    else:\n",
    "        print(f\"⚠️ LoRA not found at {LORA_PATH} - continuing without it\")\n",
    "\n",
    "    # Optimizations\n",
    "    if DEVICE == \"cuda\":\n",
    "        pipe.enable_xformers_memory_efficient_attention()\n",
    "    \n",
    "    return pipe\n",
    "\n",
    "# ========== IMAGE GENERATION ==========\n",
    "def generate_image(pipe, input_image, prompt, negative_prompt, strength, guidance_scale):\n",
    "    \"\"\"Generate a single img2img output\"\"\"\n",
    "    input_image = input_image.resize((512, 512))  # SD works best at 512x512\n",
    "    with torch.autocast(DEVICE) if DEVICE == \"cuda\" else torch.no_grad():\n",
    "        return pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            image=input_image,\n",
    "            strength=strength,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=NUM_STEPS\n",
    "        ).images[0]\n",
    "\n",
    "def process_folder(pipe):\n",
    "    \"\"\"Process all images in input folder\"\"\"\n",
    "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "    image_files = [f for f in os.listdir(INPUT_FOLDER) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    print(f\"🔍 Found {len(image_files)} images in {INPUT_FOLDER}\")\n",
    "    for image_file in tqdm(image_files, desc=\"Generating images\"):\n",
    "        input_path = os.path.join(INPUT_FOLDER, image_file)\n",
    "        try:\n",
    "            base_image = Image.open(input_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {image_file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        # Generate variations\n",
    "        for i in range(NUM_VARIATIONS):\n",
    "            output_name = f\"{os.path.splitext(image_file)[0]}_lora_{i+1}.png\"\n",
    "            output_path = os.path.join(OUTPUT_FOLDER, output_name)\n",
    "            \n",
    "            if os.path.exists(output_path):\n",
    "                print(f\"⏩ Skipping existing: {output_name}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                result = generate_image(\n",
    "                    pipe=pipe,\n",
    "                    input_image=base_image,\n",
    "                    prompt=PROMPT,\n",
    "                    negative_prompt=NEGATIVE_PROMPT,\n",
    "                    strength=STRENGTH,\n",
    "                    guidance_scale=GUIDANCE_SCALE\n",
    "                )\n",
    "                result.save(output_path)\n",
    "                print(f\"✅ Saved: {output_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed on {image_file} (variation {i+1}): {str(e)}\")\n",
    "\n",
    "# ========== MAIN ==========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting pipeline...\")\n",
    "    pipe = setup_pipeline()\n",
    "    process_folder(pipe)\n",
    "    print(\"🎉 All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816dd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffusers : 0.33.1\n",
      "peft : 0.10.0\n",
      "transformers : 4.51.2\n"
     ]
    }
   ],
   "source": [
    "import diffusers\n",
    "import peft\n",
    "import transformers\n",
    "\n",
    "print(\"diffusers :\", diffusers.__version__)\n",
    "print(\"peft :\", peft.__version__)\n",
    "print(\"transformers :\", transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe0dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00, 17.97it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Found 184 images in C:\\Users\\Amina\\Desktop\\severe pain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [04:08<00:00, 11.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 001_jpg.rf.a6e4c53c2f970d5e9547e2bdd2097616_variation_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4/22 [01:16<05:44, 19.15s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 Starting pipeline...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m pipe \u001b[38;5;241m=\u001b[39m setup_pipeline()\n\u001b[1;32m---> 92\u001b[0m \u001b[43mprocess_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎉 All done!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 75\u001b[0m, in \u001b[0;36mprocess_folder\u001b[1;34m(pipe)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPROMPT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNEGATIVE_PROMPT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTRENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGUIDANCE_SCALE\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     result\u001b[38;5;241m.\u001b[39msave(output_path)\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 42\u001b[0m, in \u001b[0;36mgenerate_image\u001b[1;34m(pipe, input_image, prompt, negative_prompt, strength, guidance_scale)\u001b[0m\n\u001b[0;32m     40\u001b[0m input_image \u001b[38;5;241m=\u001b[39m input_image\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m))  \u001b[38;5;66;03m# SD works best at 512x512\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(DEVICE) \u001b[38;5;28;01mif\u001b[39;00m DEVICE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_STEPS\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\pipelines\\stable_diffusion\\pipeline_stable_diffusion_img2img.py:1101\u001b[0m, in \u001b[0;36mStableDiffusionImg2ImgPipeline.__call__\u001b[1;34m(self, prompt, image, strength, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m latent_model_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mscale_model_input(latent_model_input, t)\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;66;03m# predict the noise residual\u001b[39;00m\n\u001b[1;32m-> 1101\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_classifier_free_guidance:\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\models\\unets\\unet_2d_condition.py:1279\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[1;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     upsample_size \u001b[38;5;241m=\u001b[39m down_block_res_samples[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(upsample_block, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_cross_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m upsample_block\u001b[38;5;241m.\u001b[39mhas_cross_attention:\n\u001b[1;32m-> 1279\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mupsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mres_hidden_states_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupsample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupsample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1290\u001b[0m     sample \u001b[38;5;241m=\u001b[39m upsample_block(\n\u001b[0;32m   1291\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39msample,\n\u001b[0;32m   1292\u001b[0m         temb\u001b[38;5;241m=\u001b[39memb,\n\u001b[0;32m   1293\u001b[0m         res_hidden_states_tuple\u001b[38;5;241m=\u001b[39mres_samples,\n\u001b[0;32m   1294\u001b[0m         upsample_size\u001b[38;5;241m=\u001b[39mupsample_size,\n\u001b[0;32m   1295\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\models\\unets\\unet_2d_blocks.py:2458\u001b[0m, in \u001b[0;36mCrossAttnUpBlock2D.forward\u001b[1;34m(self, hidden_states, res_hidden_states_tuple, temb, encoder_hidden_states, cross_attention_kwargs, upsample_size, attention_mask, encoder_attention_mask)\u001b[0m\n\u001b[0;32m   2456\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2457\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb)\n\u001b[1;32m-> 2458\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2459\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2460\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2461\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2462\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2463\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2464\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2465\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   2467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers:\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\models\\transformers\\transformer_2d.py:427\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[0;32m    416\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    417\u001b[0m             block,\n\u001b[0;32m    418\u001b[0m             hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    424\u001b[0m             class_labels,\n\u001b[0;32m    425\u001b[0m         )\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;66;03m# 3. Output\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_continuous:\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\models\\attention.py:514\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[0m\n\u001b[0;32m    511\u001b[0m cross_attention_kwargs \u001b[38;5;241m=\u001b[39m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m cross_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    512\u001b[0m gligen_kwargs \u001b[38;5;241m=\u001b[39m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgligen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 514\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn1(\n\u001b[0;32m    515\u001b[0m     norm_hidden_states,\n\u001b[0;32m    516\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monly_cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    517\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs,\n\u001b[0;32m    519\u001b[0m )\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mada_norm_zero\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    522\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m gate_msa\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m attn_output\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\models\\attention_processor.py:605\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    601\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_attention_kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are not expected by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    602\u001b[0m     )\n\u001b[0;32m    603\u001b[0m cross_attention_kwargs \u001b[38;5;241m=\u001b[39m {k: w \u001b[38;5;28;01mfor\u001b[39;00m k, w \u001b[38;5;129;01min\u001b[39;00m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m attn_parameters}\n\u001b[1;32m--> 605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    607\u001b[0m     hidden_states,\n\u001b[0;32m    608\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    609\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs,\n\u001b[0;32m    611\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\diffusers\\models\\attention_processor.py:3288\u001b[0m, in \u001b[0;36mAttnProcessor2_0.__call__\u001b[1;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn\u001b[38;5;241m.\u001b[39mgroup_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3286\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mgroup_norm(hidden_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m-> 3288\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3291\u001b[0m     encoder_hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amina\\sd-env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"  # Base model\n",
    "\n",
    "# Image generation settings\n",
    "PROMPT = \"a highly detailed and realistic photo of a patient in severe pain, hospital environment\"\n",
    "NEGATIVE_PROMPT = \"blurry, distorted face, bad anatomy, extra limbs, text, watermark, low quality\"\n",
    "STRENGTH = 0.75      # How much to alter the input image (0 = no change, 1 = complete transformation)\n",
    "GUIDANCE_SCALE = 7.5  # How closely to follow the prompt\n",
    "NUM_STEPS = 30        # Inference steps (20-50 is typical)\n",
    "\n",
    "# Paths (use raw strings or forward slashes)\n",
    "INPUT_FOLDER = r\"C:\\Users\\Amina\\Desktop\\severe pain\"\n",
    "OUTPUT_FOLDER = r\"C:\\Users\\Amina\\Desktop\\severe pain\\generated_lora\"\n",
    "NUM_VARIATIONS = 3    # Number of outputs per input image\n",
    "\n",
    "# ========== SETUP PIPELINE ==========\n",
    "def setup_pipeline():\n",
    "    # Load base model\n",
    "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "        safety_checker=None,  # Disable if you get NSFW filter false positives\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Optimizations\n",
    "    if DEVICE == \"cuda\":\n",
    "        pipe.enable_xformers_memory_efficient_attention()\n",
    "    \n",
    "    return pipe\n",
    "\n",
    "# ========== IMAGE GENERATION ==========\n",
    "def generate_image(pipe, input_image, prompt, negative_prompt, strength, guidance_scale):\n",
    "    \"\"\"Generate a single img2img output\"\"\"\n",
    "    input_image = input_image.resize((512, 512))  # SD works best at 512x512\n",
    "    with torch.autocast(DEVICE) if DEVICE == \"cuda\" else torch.no_grad():\n",
    "        return pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            image=input_image,\n",
    "            strength=strength,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=NUM_STEPS\n",
    "        ).images[0]\n",
    "\n",
    "def process_folder(pipe):\n",
    "    \"\"\"Process all images in input folder\"\"\"\n",
    "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "    image_files = [f for f in os.listdir(INPUT_FOLDER) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    print(f\"🔍 Found {len(image_files)} images in {INPUT_FOLDER}\")\n",
    "    for image_file in image_files:\n",
    "        input_path = os.path.join(INPUT_FOLDER, image_file)\n",
    "        try:\n",
    "            base_image = Image.open(input_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {image_file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        # Generate variations\n",
    "        for i in range(NUM_VARIATIONS):\n",
    "            output_name = f\"{os.path.splitext(image_file)[0]}_variation_{i+1}.png\"\n",
    "            output_path = os.path.join(OUTPUT_FOLDER, output_name)\n",
    "            \n",
    "            if os.path.exists(output_path):\n",
    "                print(f\"⏩ Skipping existing: {output_name}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                result = generate_image(\n",
    "                    pipe=pipe,\n",
    "                    input_image=base_image,\n",
    "                    prompt=PROMPT,\n",
    "                    negative_prompt=NEGATIVE_PROMPT,\n",
    "                    strength=STRENGTH,\n",
    "                    guidance_scale=GUIDANCE_SCALE\n",
    "                )\n",
    "                result.save(output_path)\n",
    "                print(f\"✅ Saved: {output_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed on {image_file} (variation {i+1}): {str(e)}\")\n",
    "\n",
    "# ========== MAIN ==========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting pipeline...\")\n",
    "    pipe = setup_pipeline()\n",
    "    process_folder(pipe)\n",
    "    print(\"🎉 All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981548b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00, 12.90it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Found 184 images in C:\\Users\\Amina\\Desktop\\severe pain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [06:00<00:00, 11.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 001_jpg.rf.a6e4c53c2f970d5e9547e2bdd2097616_realistic_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [1:01:20<00:00, 115.02s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 001_jpg.rf.a6e4c53c2f970d5e9547e2bdd2097616_realistic_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [08:29<00:00, 15.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 001_jpg.rf.a6e4c53c2f970d5e9547e2bdd2097616_realistic_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [09:00<00:00, 16.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 001_jpg.rf.a6e4c53c2f970d5e9547e2bdd2097616_realistic_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [09:07<00:00, 17.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 002_jpg.rf.325a0d02653bd6e60a00784f485920ce_realistic_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [09:27<00:00, 17.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 002_jpg.rf.325a0d02653bd6e60a00784f485920ce_realistic_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [09:11<00:00, 17.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 002_jpg.rf.325a0d02653bd6e60a00784f485920ce_realistic_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [08:44<00:00, 16.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 002_jpg.rf.325a0d02653bd6e60a00784f485920ce_realistic_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [08:25<00:00, 15.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 003_jpg.rf.0b581e515c39c9539d2b03811b4fafbc_realistic_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [1:34:09<00:00, 176.54s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 003_jpg.rf.0b581e515c39c9539d2b03811b4fafbc_realistic_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [08:33<00:00, 16.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 003_jpg.rf.0b581e515c39c9539d2b03811b4fafbc_realistic_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [06:42<00:00, 12.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 003_jpg.rf.0b581e515c39c9539d2b03811b4fafbc_realistic_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [06:08<00:00, 11.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 005_jpg.rf.7f1eeb63718cbce5f7454fd40b2d4c98_realistic_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [05:53<00:00, 11.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 005_jpg.rf.7f1eeb63718cbce5f7454fd40b2d4c98_realistic_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [05:57<00:00, 11.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 005_jpg.rf.7f1eeb63718cbce5f7454fd40b2d4c98_realistic_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [06:19<00:00, 11.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 005_jpg.rf.7f1eeb63718cbce5f7454fd40b2d4c98_realistic_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [05:51<00:00, 10.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 008_jpg.rf.f3fc4d46734679ccc4b09c5d4fa375ee_realistic_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [29:18<00:00, 54.95s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 008_jpg.rf.f3fc4d46734679ccc4b09c5d4fa375ee_realistic_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [06:24<00:00, 12.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 008_jpg.rf.f3fc4d46734679ccc4b09c5d4fa375ee_realistic_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [08:11<00:00, 15.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 008_jpg.rf.f3fc4d46734679ccc4b09c5d4fa375ee_realistic_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [10:19:07<00:00, 1160.86s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 009_jpg.rf.7034924a468a7d351b3240a032e7deb6_realistic_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [06:48<00:00, 12.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 009_jpg.rf.7034924a468a7d351b3240a032e7deb6_realistic_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [07:26<00:00, 13.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 009_jpg.rf.7034924a468a7d351b3240a032e7deb6_realistic_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [08:38<00:00, 16.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 009_jpg.rf.7034924a468a7d351b3240a032e7deb6_realistic_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [3:41:19<00:00, 414.97s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 010_jpg.rf.1a3dac6e5ff6d4aaf574ba0698fca038_realistic_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [10:22<00:00, 19.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 010_jpg.rf.1a3dac6e5ff6d4aaf574ba0698fca038_realistic_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [09:20<00:00, 17.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 010_jpg.rf.1a3dac6e5ff6d4aaf574ba0698fca038_realistic_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [09:38<00:00, 18.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 010_jpg.rf.1a3dac6e5ff6d4aaf574ba0698fca038_realistic_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [09:19<00:00, 17.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 015_jpg.rf.bbe3075edc3a7f6c6be842fc8ccea001_realistic_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [09:25<00:00, 17.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 015_jpg.rf.bbe3075edc3a7f6c6be842fc8ccea001_realistic_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [09:04<00:00, 17.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 015_jpg.rf.bbe3075edc3a7f6c6be842fc8ccea001_realistic_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [10:08<00:00, 19.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 015_jpg.rf.bbe3075edc3a7f6c6be842fc8ccea001_realistic_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [09:27<00:00, 17.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 015_jpg.rf.f13e420e0f4b4698c640a68a0ff8ef83_realistic_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [10:21<00:00, 19.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 015_jpg.rf.f13e420e0f4b4698c640a68a0ff8ef83_realistic_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [09:07<00:00, 17.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 015_jpg.rf.f13e420e0f4b4698c640a68a0ff8ef83_realistic_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [08:40<00:00, 16.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 015_jpg.rf.f13e420e0f4b4698c640a68a0ff8ef83_realistic_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [10:27<00:00, 19.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 017_jpg.rf.0a7a65068bbcf99752f7862083dba45b_realistic_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [10:34<00:00, 19.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 017_jpg.rf.0a7a65068bbcf99752f7862083dba45b_realistic_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [10:14<00:00, 19.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 017_jpg.rf.0a7a65068bbcf99752f7862083dba45b_realistic_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [09:56<00:00, 18.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 017_jpg.rf.0a7a65068bbcf99752f7862083dba45b_realistic_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [08:14<00:00, 15.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 020_jpg.rf.c8a73802e0b435ea5dd9e040be9d30b5_realistic_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [08:04<00:00, 15.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 020_jpg.rf.c8a73802e0b435ea5dd9e040be9d30b5_realistic_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 22/32 [06:28<04:23, 26.32s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"  # Base model\n",
    "\n",
    "# Advanced Image Generation Settings for Realism\n",
    "PROMPT = \"a hyper-realistic, emotionally intense photo of a patient in severe pain, hospital lighting, close-up, detailed facial expressions\"\n",
    "NEGATIVE_PROMPT = \"blurry, distorted face, low quality, excessive artifacts, watermarks, extra limbs, bad anatomy\"\n",
    "STRENGTH = 0.65       # Moderate transformation to maintain realism\n",
    "GUIDANCE_SCALE = 12.0  # Higher value for more adherence to the prompt\n",
    "NUM_STEPS = 50        # More inference steps for finer details\n",
    "\n",
    "# Paths (use raw strings or forward slashes)\n",
    "INPUT_FOLDER = r\"C:\\Users\\Amina\\Desktop\\severe pain\"  # Update to your actual input folder\n",
    "OUTPUT_FOLDER = r\"C:\\Users\\Amina\\Desktop\\severe pain\\generated_lora\"   # Update to your desired output folder\n",
    "NUM_VARIATIONS = 4    # Number of variations per input image\n",
    "\n",
    "# ========== SETUP PIPELINE ==========\n",
    "def setup_pipeline():\n",
    "    # Load the pre-trained Stable Diffusion model\n",
    "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "        safety_checker=None,  # Disable safety filter if it causes false positives\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Optimizations\n",
    "    if DEVICE == \"cuda\":\n",
    "        pipe.enable_xformers_memory_efficient_attention()\n",
    "    \n",
    "    return pipe\n",
    "\n",
    "# ========== IMAGE GENERATION ==========\n",
    "def generate_image(pipe, input_image, prompt, negative_prompt, strength, guidance_scale):\n",
    "    \"\"\"Generate a high-quality img2img output\"\"\"\n",
    "    input_image = input_image.resize((512, 512))  # Resize for best SD performance\n",
    "    with torch.autocast(DEVICE) if DEVICE == \"cuda\" else torch.no_grad():\n",
    "        # Generate the image with higher detail settings\n",
    "        result = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            image=input_image,\n",
    "            strength=strength,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=NUM_STEPS\n",
    "        ).images[0]\n",
    "    return result\n",
    "\n",
    "def process_folder(pipe):\n",
    "    \"\"\"Process all images in the input folder to generate realistic images\"\"\"\n",
    "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "    image_files = [f for f in os.listdir(INPUT_FOLDER) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    print(f\"🔍 Found {len(image_files)} images in {INPUT_FOLDER}\")\n",
    "    for image_file in image_files:\n",
    "        input_path = os.path.join(INPUT_FOLDER, image_file)\n",
    "        try:\n",
    "            base_image = Image.open(input_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {image_file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        # Generate variations with more details\n",
    "        for i in range(NUM_VARIATIONS):\n",
    "            output_name = f\"{os.path.splitext(image_file)[0]}_realistic_{i+1}.png\"\n",
    "            output_path = os.path.join(OUTPUT_FOLDER, output_name)\n",
    "\n",
    "            if os.path.exists(output_path):\n",
    "                print(f\"⏩ Skipping existing: {output_name}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                result = generate_image(\n",
    "                    pipe=pipe,\n",
    "                    input_image=base_image,\n",
    "                    prompt=PROMPT,\n",
    "                    negative_prompt=NEGATIVE_PROMPT,\n",
    "                    strength=STRENGTH,\n",
    "                    guidance_scale=GUIDANCE_SCALE\n",
    "                )\n",
    "                result.save(output_path)\n",
    "                print(f\"✅ Saved: {output_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed on {image_file} (variation {i+1}): {str(e)}\")\n",
    "\n",
    "# ========== MAIN ==========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting pipeline...\")\n",
    "    pipe = setup_pipeline()\n",
    "    process_folder(pipe)\n",
    "    print(\"🎉 All done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sd-env)",
   "language": "python",
   "name": "sd-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
