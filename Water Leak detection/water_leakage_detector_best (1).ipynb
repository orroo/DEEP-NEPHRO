{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p7uza1f4ctA9",
    "outputId": "db1b4cf7-e952-463b-ca3d-be6698cd9f53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training samples: 650\n",
      "Validation samples: 153\n",
      "Test samples: 154\n",
      "Class weights: tensor([1.8494, 0.6853])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 [Train]: 100%|██████████| 21/21 [08:15<00:00, 23.61s/it, accuracy=78.2, loss=0.0959, lr=0.001] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Train Loss: 0.7326, Acc: 78.15% - Val Loss: 0.0047, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 [Train]: 100%|██████████| 21/21 [00:23<00:00,  1.11s/it, accuracy=96.6, loss=0.000437, lr=0.000976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 - Train Loss: 0.0228, Acc: 96.62% - Val Loss: 0.0055, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.27it/s, accuracy=97.5, loss=1.4e-6, lr=0.000905] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 - Train Loss: 0.0136, Acc: 97.54% - Val Loss: 0.0044, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.30it/s, accuracy=97.4, loss=0.000445, lr=0.000796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 - Train Loss: 0.0098, Acc: 97.38% - Val Loss: 0.0053, Acc: 98.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.30it/s, accuracy=97.5, loss=0.000622, lr=0.000658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 - Train Loss: 0.0104, Acc: 97.54% - Val Loss: 0.0045, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.30it/s, accuracy=96.9, loss=0.00351, lr=0.000505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 - Train Loss: 0.0104, Acc: 96.92% - Val Loss: 0.0047, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.30it/s, accuracy=97.4, loss=0.000359, lr=0.000352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 - Train Loss: 0.0080, Acc: 97.38% - Val Loss: 0.0039, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.30it/s, accuracy=97.2, loss=0.00338, lr=0.000214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 - Train Loss: 0.0090, Acc: 97.23% - Val Loss: 0.0039, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.29it/s, accuracy=97.4, loss=0.000945, lr=0.000105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 - Train Loss: 0.0069, Acc: 97.38% - Val Loss: 0.0039, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.31it/s, accuracy=97.2, loss=0.000909, lr=3.42e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 - Train Loss: 0.0071, Acc: 97.23% - Val Loss: 0.0035, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.30it/s, accuracy=97.4, loss=0.00164, lr=0.001] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 - Train Loss: 0.0076, Acc: 97.38% - Val Loss: 0.0036, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.31it/s, accuracy=97.5, loss=0.000633, lr=0.000994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 - Train Loss: 0.0091, Acc: 97.54% - Val Loss: 0.0037, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.31it/s, accuracy=97.2, loss=6.48e-5, lr=0.000976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 - Train Loss: 0.0077, Acc: 97.23% - Val Loss: 0.0038, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.31it/s, accuracy=97.7, loss=0.000139, lr=0.000946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 - Train Loss: 0.0060, Acc: 97.69% - Val Loss: 0.0044, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.31it/s, accuracy=98, loss=7.58e-5, lr=0.000905]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 - Train Loss: 0.0069, Acc: 98.00% - Val Loss: 0.0055, Acc: 97.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.31it/s, accuracy=97.4, loss=0.0138, lr=0.000855] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 - Train Loss: 0.0056, Acc: 97.38% - Val Loss: 0.0044, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.30it/s, accuracy=97.4, loss=2.6e-6, lr=0.000796]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 - Train Loss: 0.0044, Acc: 97.38% - Val Loss: 0.0043, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.29it/s, accuracy=97.4, loss=0.00042, lr=0.00073] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 - Train Loss: 0.0045, Acc: 97.38% - Val Loss: 0.0037, Acc: 98.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.28it/s, accuracy=97.5, loss=0.0164, lr=0.000658]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 - Train Loss: 0.0045, Acc: 97.54% - Val Loss: 0.0046, Acc: 97.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.29it/s, accuracy=97.2, loss=8.03e-6, lr=0.000582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 - Train Loss: 0.0063, Acc: 97.23% - Val Loss: 0.0041, Acc: 98.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.31it/s, accuracy=97.5, loss=0.00641, lr=0.000505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 - Train Loss: 0.0070, Acc: 97.54% - Val Loss: 0.0041, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.30it/s, accuracy=97.8, loss=0.000285, lr=0.000428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 - Train Loss: 0.0048, Acc: 97.85% - Val Loss: 0.0038, Acc: 98.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.31it/s, accuracy=97.5, loss=0.000599, lr=0.000352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 - Train Loss: 0.0044, Acc: 97.54% - Val Loss: 0.0049, Acc: 97.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.31it/s, accuracy=97.8, loss=0.000284, lr=0.00028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25 - Train Loss: 0.0035, Acc: 97.85% - Val Loss: 0.0051, Acc: 98.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 [Train]: 100%|██████████| 21/21 [00:16<00:00,  1.30it/s, accuracy=97.5, loss=0.000416, lr=0.000214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 - Train Loss: 0.0056, Acc: 97.54% - Val Loss: 0.0056, Acc: 97.39%\n",
      "Training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_25668\\2380555119.py:287: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.70%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Leak       1.00      0.95      0.98        44\n",
      "        Leak       0.98      1.00      0.99       110\n",
      "\n",
      "    accuracy                           0.99       154\n",
      "   macro avg       0.99      0.98      0.98       154\n",
      "weighted avg       0.99      0.99      0.99       154\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 42   2]\n",
      " [  0 110]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "import random\n",
    "from typing import Any, Tuple\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example transforms (you can adjust as needed)\n",
    "leak_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "\n",
    "])\n",
    "\n",
    "no_leak_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "\n",
    "])\n",
    "\n",
    "# Custom ImageFolder to apply different transforms based on label\n",
    "class CustomImageFolder(datasets.ImageFolder):\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        path, label = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if label == 0:  # Assuming 0 is no-leak\n",
    "            sample = no_leak_transform(sample)\n",
    "        else:  # Assuming 1 is leak\n",
    "            sample = leak_transform(sample)\n",
    "        return sample, label\n",
    "\n",
    "# Custom Dataset for handling class imbalance\n",
    "class BalancedDataset(Dataset):\n",
    "    def __init__(self, dataset: Dataset):\n",
    "        self.dataset = dataset\n",
    "        # Get indices for each class\n",
    "        self.no_leak_indices = [i for i, (_, label) in enumerate(dataset) if label == 0]\n",
    "        self.leak_indices = [i for i, (_, label) in enumerate(dataset) if label == 1]\n",
    "\n",
    "        # Upsample minority class (leak)\n",
    "        self.upsampled_leak_indices = random.choices(\n",
    "            self.leak_indices,\n",
    "            k=len(self.no_leak_indices)  # Match majority class size\n",
    "        )\n",
    "\n",
    "        # Combine indices\n",
    "        self.indices = self.no_leak_indices + self.upsampled_leak_indices\n",
    "        random.shuffle(self.indices)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Any, Any]:\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "class CustomImageFolder(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        path, label = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        # Transformations happen HERE based on label\n",
    "        if label == 0:\n",
    "            sample = no_leak_transform(sample)\n",
    "        else:\n",
    "            sample = leak_transform(sample)\n",
    "        return sample, label\n",
    "\n",
    "dataset = CustomImageFolder(root=r'D:\\3IA\\Leak_detection\\data')\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Create balanced training dataset\n",
    "balanced_train_dataset = BalancedDataset(train_dataset)\n",
    "train_loader = DataLoader(balanced_train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# For validation, you might want to keep original distribution\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Test DataLoader (keep original distribution)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Calculate class weights (inverse frequency)\n",
    "num_leak = len([label for _, label in dataset if label == 1])\n",
    "num_no_leak = len([label for _, label in dataset if label == 0])\n",
    "total_samples = num_leak + num_no_leak\n",
    "weight_no_leak = total_samples / (2 * num_no_leak)\n",
    "weight_leak = total_samples / (2 * num_leak)\n",
    "class_weights = torch.tensor([weight_no_leak, weight_leak])\n",
    "\n",
    "print(f\"Training samples: {len(balanced_train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class ImprovedLeakDetectionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedLeakDetectionCNN, self).__init__()\n",
    "        # Convolutional blocks with progressive dropout\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.dropout1 = nn.Dropout2d(0.2)  # Increased from 0.1\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.dropout2 = nn.Dropout2d(0.3)  # Increased from 0.2\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.dropout3 = nn.Dropout2d(0.4)  # Increased from 0.3\n",
    "        \n",
    "        # Reduced channels in final conv layer\n",
    "        self.conv4 = nn.Conv2d(128, 192, kernel_size=3, padding=1)  # Reduced from 256\n",
    "        self.bn4 = nn.BatchNorm2d(192)\n",
    "        self.dropout4 = nn.Dropout2d(0.5)  # Increased from 0.4\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers with higher dropout\n",
    "        self.fc1 = nn.Linear(192 * 8 * 8, 384)  # Reduced from 512\n",
    "        self.fc1_dropout = nn.Dropout(0.6)  # Increased from 0.5\n",
    "        self.fc2 = nn.Linear(384, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv block 1\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Conv block 2\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Conv block 3\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Conv block 4\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        # Fully connected\n",
    "        x = x.view(-1, 192 * 8 * 8)\n",
    "        x = self.fc1_dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "# Initialize with weight decay (L2 regularization)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImprovedLeakDetectionCNN().to(device)\n",
    "# 1. Enhanced Focal Loss (handles class imbalance better)\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor([alpha, 1 - alpha])  # Ensure it matches class count\n",
    "        self.gamma = gamma  \n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "\n",
    "        # Convert targets to one-hot\n",
    "        targets_onehot = F.one_hot(targets, num_classes=inputs.shape[1]).float()\n",
    "\n",
    "        # Ensure alpha is correctly shaped\n",
    "        alpha = self.alpha.to(inputs.device)[targets]  # Get per-example alpha values\n",
    "\n",
    "        loss = (alpha * (1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return loss\n",
    "\n",
    "# Adjust weights for your dataset (No-Leak: 0.6, Leak: 0.4)\n",
    "criterion = FocalLoss(alpha=0.25)\n",
    "\n",
    "\n",
    "# 2. Optimizer with stronger L2 regularization\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-3  # Increased regularization strength\n",
    ")\n",
    "\n",
    "# 3. Cosine Annealing with Warm Restarts (better than basic CosineAnnealing)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=10,           # Number of epochs for first restart\n",
    "    T_mult=2,         # Factor to increase T_0 after each restart\n",
    "    eta_min=1e-5      # Minimum learning rate\n",
    ")\n",
    "\n",
    "# 4. Early Stopping Implementation (add this class)\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "# The training loop remains the same as before\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        train_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for inputs, labels in train_progress:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            train_progress.set_postfix(\n",
    "                loss=loss.item(),\n",
    "                accuracy=100.*correct/total,\n",
    "                lr=optimizer.param_groups[0]['lr']\n",
    "            )\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = 100. * correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - '\n",
    "              f'Train Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}% - '\n",
    "              f'Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'epoch': epoch\n",
    "            }, 'best_model.pth')\n",
    "\n",
    "    print('Training complete')\n",
    "    checkpoint = torch.load('best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    loss = running_loss / len(data_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return loss, accuracy\n",
    "\n",
    "# Test function\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    # You can add more metrics here (precision, recall, F1, confusion matrix)\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['No Leak', 'Leak']))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25)\n",
    "\n",
    "# Test the model\n",
    "test_accuracy = test_model(trained_model, test_loader)\n",
    "\n",
    "# Optionally: Load the best model for testing\n",
    "# best_model = LeakDetectionCNN().to(device)\n",
    "# best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "# test_accuracy = test_model(best_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"water_leakage_detector_best.pth\")\n",
    "print(\"Model trained and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0057, Train Acc: 97.38%\n",
      "Val Loss: 0.0041, Val Acc: 98.69%\n",
      "Test Loss: 0.0039, Test Acc: 98.70%\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc = evaluate_model(model, train_loader, criterion)\n",
    "val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
    "test_loss, test_acc = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
